# AI Evaluation Portfolio â€” Tiffany M. Graves

## Overview
AI Data Annotation and Evaluation Specialist with experience in
human-in-the-loop AI evaluation, multimodal quality assurance,
transcription and captioning review, and federal-grade visual
interpretation workflows.

This portfolio demonstrates evaluation methodology, rubric-based
judgment, and failure-mode analysis using synthetic, anonymized
examples suitable for public review.

## Evaluation Domains
- LLM response evaluation (helpfulness, correctness, tone, safety)
- Human-to-Human (H2H) comparison and preference ranking
- Multimodal QA (image, video, UI annotations)
- Transcription and captioning quality review
- Bias, hallucination, and instruction-following checks

## Evaluation Approach
Evaluations are conducted using clearly defined criteria, severity
levels, and written justification. Emphasis is placed on:
- instruction adherence
- user safety and risk awareness
- factual grounding and hallucination detection
- consistency and audit readiness

## Tools & Modalities (non-exhaustive)
Text, image, video, UI, and conversational data evaluated using
web-based annotation platforms and professional QA tools, including
Subtitle Edit, Aegisub, OOONA Toolkit, Rev Editor, Power BI, Tableau,
KoboToolbox, ODK/XLSForm, ArcGIS Pro, and ERDAS Imagine.

## Compliance & Ethics
All examples in this repository are synthetic, self-generated, or
de-identified. No NDA-protected prompts, platform-specific guidelines,
client data, or proprietary tooling screenshots are included.

## Case Studies
- LLM Response Evaluation  
- H2H Comparison  
- Transcription & Captioning QA  
- Multimodal Visual QA  

Each case study focuses on evaluation reasoning and justification,
not content generation.

## Contact
LinkedIn: linkedin.com/in/tiffanymgraves  
Email: tmgraves04@gmail.com
